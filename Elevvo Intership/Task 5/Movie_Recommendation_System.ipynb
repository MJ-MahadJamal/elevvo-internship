{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwHh41ST2Mqp",
        "outputId": "2c774845-ca03-4e15-d9c3-2dafa1b35b9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path: /kaggle/input/movielens-100k-dataset\n",
            "Ratings shape: (100000, 5) Movies shape: (1682, 2)\n",
            "Active users: 943\n",
            "Users in both splits: 943\n",
            "Matrix shape (train): 943 users x 1646 items\n",
            "\n",
            "Tuning User-CF...\n",
            "User-CF (k=20, alpha=0.0) — Avg Precision@5: 0.2250  (users=937)\n",
            "User-CF (k=20, alpha=0.2) — Avg Precision@5: 0.2226  (users=937)\n",
            "User-CF (k=20, alpha=0.4) — Avg Precision@5: 0.2194  (users=937)\n",
            "User-CF (k=50, alpha=0.0) — Avg Precision@5: 0.2399  (users=937)\n",
            "User-CF (k=50, alpha=0.2) — Avg Precision@5: 0.2329  (users=937)\n",
            "User-CF (k=50, alpha=0.4) — Avg Precision@5: 0.2235  (users=937)\n",
            "User-CF (k=100, alpha=0.0) — Avg Precision@5: 0.2309  (users=937)\n",
            "User-CF (k=100, alpha=0.2) — Avg Precision@5: 0.2267  (users=937)\n",
            "User-CF (k=100, alpha=0.4) — Avg Precision@5: 0.2175  (users=937)\n",
            "Best User-CF: P@5=0.2399, k=50, alpha=0.0\n",
            "\n",
            "Tuning Item-CF...\n",
            "Item-CF (k=20, alpha=0.0) — Avg Precision@5: 0.0608  (users=937)\n",
            "Item-CF (k=20, alpha=0.2) — Avg Precision@5: 0.0683  (users=937)\n",
            "Item-CF (k=20, alpha=0.4) — Avg Precision@5: 0.0835  (users=937)\n",
            "Item-CF (k=50, alpha=0.0) — Avg Precision@5: 0.0608  (users=937)\n",
            "Item-CF (k=50, alpha=0.2) — Avg Precision@5: 0.0683  (users=937)\n",
            "Item-CF (k=50, alpha=0.4) — Avg Precision@5: 0.0835  (users=937)\n",
            "Item-CF (k=100, alpha=0.0) — Avg Precision@5: 0.0608  (users=937)\n",
            "Item-CF (k=100, alpha=0.2) — Avg Precision@5: 0.0683  (users=937)\n",
            "Item-CF (k=100, alpha=0.4) — Avg Precision@5: 0.0835  (users=937)\n",
            "Best Item-CF: P@5=0.0835, k=20, alpha=0.4\n",
            "\n",
            "Evaluating SVD (MF)...\n",
            "SVD (MF) — Avg Precision@5: 0.1330  (users=937)\n",
            "\n",
            "=== Summary (best settings) ===\n",
            "User-CF best: P@5=0.2399, neighbors=50, alpha=0.0\n",
            "Item-CF best: P@5=0.0835, neighbors=20, alpha=0.4\n",
            "SVD (MF):     P@5=0.1330\n",
            "\n",
            "Tip: Increase MIN_RATINGS_PER_USER (e.g., 30), expand NEIGHBOR_GRID, and include alpha=0.6–0.8 for stronger popularity blending if you want even higher P@K.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Task 5: Movie Recommendation System (MovieLens 100K)\n",
        "\n",
        "!pip install -q kagglehub numpy pandas scikit-learn\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from scipy import sparse\n",
        "import kagglehub\n",
        "\n",
        "# Config\n",
        "\n",
        "K = 5                       # Precision@K\n",
        "LIKE_THRESHOLD = 4          # Relevant if rating >= 4\n",
        "MIN_RATINGS_PER_USER = 20   # Filter sparse users for more stable eval\n",
        "TEST_FRACTION_PER_USER = 0.2  # 20% per user -> test\n",
        "\n",
        "USE_USER_CF = True\n",
        "USE_ITEM_CF = True\n",
        "USE_SVD     = True\n",
        "\n",
        "# Small \"tuning\" grids (kept tiny for speed, but effective)\n",
        "NEIGHBOR_GRID = [20, 50, 100]\n",
        "ALPHA_GRID    = [0.0, 0.2, 0.4]  # popularity blend weight (0 = pure CF)\n",
        "\n",
        "\n",
        "# Download & Load\n",
        "\n",
        "path = kagglehub.dataset_download(\"prajitdatta/movielens-100k-dataset\")\n",
        "print(\"Path:\", path)\n",
        "\n",
        "ratings_path = f\"{path}/ml-100k/u.data\"\n",
        "movies_path  = f\"{path}/ml-100k/u.item\"\n",
        "\n",
        "ratings = pd.read_csv(\n",
        "    ratings_path, sep=\"\\t\",\n",
        "    names=[\"user_id\", \"item_id\", \"rating\", \"timestamp\"]\n",
        ")\n",
        "\n",
        "movies = pd.read_csv(\n",
        "    movies_path, sep=\"|\", encoding=\"latin-1\",\n",
        "    header=None, usecols=[0, 1], names=[\"item_id\", \"title\"]\n",
        ")\n",
        "\n",
        "ratings = ratings.merge(movies, on=\"item_id\", how=\"left\")\n",
        "print(\"Ratings shape:\", ratings.shape, \"Movies shape:\", movies.shape)\n",
        "\n",
        "\n",
        "# Filter sparse users\n",
        "\n",
        "user_counts = ratings.groupby(\"user_id\")[\"item_id\"].count()\n",
        "active_users = user_counts[user_counts >= MIN_RATINGS_PER_USER].index\n",
        "ratings = ratings[ratings[\"user_id\"].isin(active_users)].copy()\n",
        "print(\"Active users:\", ratings[\"user_id\"].nunique())\n",
        "\n",
        "# Per-user train/test split (stratified by user)\n",
        "\n",
        "rng = np.random.default_rng(42)\n",
        "train_list, test_list = [], []\n",
        "\n",
        "for u, grp in ratings.groupby(\"user_id\"):\n",
        "    idx = grp.index.to_numpy()\n",
        "    n_test = max(1, int(np.ceil(TEST_FRACTION_PER_USER * len(idx))))\n",
        "    test_idx = rng.choice(idx, size=n_test, replace=False)\n",
        "    train_idx = np.setdiff1d(idx, test_idx, assume_unique=True)\n",
        "    train_list.append(ratings.loc[train_idx])\n",
        "    test_list.append(ratings.loc[test_idx])\n",
        "\n",
        "train = pd.concat(train_list, ignore_index=True)\n",
        "test  = pd.concat(test_list, ignore_index=True)\n",
        "\n",
        "# Only evaluate users in both splits\n",
        "common_users = np.intersect1d(train[\"user_id\"].unique(), test[\"user_id\"].unique())\n",
        "train = train[train[\"user_id\"].isin(common_users)]\n",
        "test  = test[test[\"user_id\"].isin(common_users)]\n",
        "print(\"Users in both splits:\", len(common_users))\n",
        "\n",
        "# Restrict items to those seen in training (prevents leakage)\n",
        "all_items = np.sort(train[\"item_id\"].unique())\n",
        "train = train[train[\"item_id\"].isin(all_items)]\n",
        "test  = test[test[\"item_id\"].isin(all_items)]\n",
        "\n",
        "\n",
        "# Build ID maps and sparse train matrix\n",
        "\n",
        "all_users = np.sort(train[\"user_id\"].unique())\n",
        "user_to_idx = {u:i for i,u in enumerate(all_users)}\n",
        "idx_to_user = {i:u for u,i in user_to_idx.items()}\n",
        "item_to_idx = {m:i for i,m in enumerate(all_items)}\n",
        "idx_to_item = {i:m for m,i in item_to_idx.items()}\n",
        "\n",
        "n_users = len(all_users)\n",
        "n_items = len(all_items)\n",
        "print(f\"Matrix shape (train): {n_users} users x {n_items} items\")\n",
        "\n",
        "def df_to_csr(df, n_users, n_items, user_to_idx, item_to_idx):\n",
        "    rows = df[\"user_id\"].map(user_to_idx)\n",
        "    cols = df[\"item_id\"].map(item_to_idx)\n",
        "    mask = rows.notna() & cols.notna()\n",
        "    rows = rows[mask].astype(int)\n",
        "    cols = cols[mask].astype(int)\n",
        "    vals = df.loc[mask, \"rating\"].astype(float)\n",
        "    return sparse.csr_matrix((vals, (rows, cols)), shape=(n_users, n_items))\n",
        "\n",
        "R_train = df_to_csr(train, n_users, n_items, user_to_idx, item_to_idx)\n",
        "\n",
        "# Precompute per-user seen sets (as indices)\n",
        "user_seen_items = [set(train[train[\"user_id\"]==idx_to_user[u]][\"item_id\"].map(item_to_idx).dropna().astype(int))\n",
        "                   for u in range(n_users)]\n",
        "\n",
        "# Test relevant items per user (set of item ids with rating >= threshold)\n",
        "test_eval = test[test[\"item_id\"].isin(all_items)].copy()\n",
        "test_relevant = (\n",
        "    test_eval[test_eval[\"rating\"] >= LIKE_THRESHOLD]\n",
        "    .groupby(\"user_id\")[\"item_id\"].apply(set)\n",
        ")\n",
        "\n",
        "# Popularity (train) — average rating * count boost\n",
        "movie_group = train.groupby(\"item_id\")[\"rating\"]\n",
        "pop_mean = movie_group.mean()\n",
        "pop_cnt  = movie_group.count()\n",
        "# soft popularity score\n",
        "pop_score = (pop_mean - pop_mean.min()) / (pop_mean.max() - pop_mean.min() + 1e-9) * np.log1p(pop_cnt)\n",
        "# normalize to [0,1]\n",
        "pop_norm = (pop_score - pop_score.min()) / (pop_score.max() - pop_score.min() + 1e-9)\n",
        "pop_vec = np.zeros(n_items)\n",
        "for iid, score in pop_norm.items():\n",
        "    if iid in item_to_idx:\n",
        "        pop_vec[item_to_idx[iid]] = score\n",
        "\n",
        "# Evaluation helpers\n",
        "def precision_at_k_single(rec_indices, relevant_item_ids, k=K):\n",
        "    if not relevant_item_ids:\n",
        "        return None\n",
        "    rec_ids = [idx_to_item[i] for i in rec_indices[:k]]\n",
        "    hits = len(set(rec_ids) & relevant_item_ids)\n",
        "    return hits / k\n",
        "\n",
        "def evaluate_precision(recommender_fn, label, k=K):\n",
        "    scores = []\n",
        "    for u in common_users:\n",
        "        if u not in test_relevant:\n",
        "            continue\n",
        "        u_idx = user_to_idx[u]\n",
        "        rec_idx = recommender_fn(u_idx, k)\n",
        "        p = precision_at_k_single(rec_idx, test_relevant[u], k)\n",
        "        if p is not None:\n",
        "            scores.append(p)\n",
        "    mean_p = float(np.mean(scores)) if scores else 0.0\n",
        "    print(f\"{label} — Avg Precision@{k}: {mean_p:.4f}  (users={len(scores)})\")\n",
        "    return mean_p\n",
        "\n",
        "# Core: mean-centered utility\n",
        "\n",
        "# Dense copy (ML-100k is small; fine in Colab)\n",
        "R_dense = R_train.toarray().astype(np.float32)\n",
        "# User mean-centering\n",
        "user_means = np.where(R_dense.sum(axis=1) > 0, np.divide(R_dense.sum(axis=1), (R_dense > 0).sum(axis=1)), 0.0)\n",
        "R_uc = R_dense.copy()\n",
        "mask = (R_uc > 0)\n",
        "R_uc[mask] = (R_uc[mask] - np.repeat(user_means[:, None], n_items, axis=1)[mask])\n",
        "\n",
        "# Item mean-centering\n",
        "item_means = np.where(R_dense.sum(axis=0) > 0, np.divide(R_dense.sum(axis=0), (R_dense > 0).sum(axis=0)), 0.0)\n",
        "R_ic = R_dense.copy()\n",
        "mask_i = (R_ic > 0)\n",
        "R_ic[mask_i] = (R_ic[mask_i] - np.repeat(item_means[None, :], n_users, axis=0)[mask_i])\n",
        "\n",
        "# Normalize for cosine\n",
        "R_uc_norm = normalize(R_uc, norm=\"l2\", axis=1, copy=True)\n",
        "R_ic_norm = normalize(R_ic, norm=\"l2\", axis=0, copy=True)\n",
        "\n",
        "# Similarities\n",
        "user_sim = (R_uc_norm @ R_uc_norm.T)   # (n_users x n_users)\n",
        "item_sim = (R_ic_norm.T @ R_ic_norm)   # (n_items x n_items)\n",
        "\n",
        "# Recommenders (with tuning)\n",
        "\n",
        "def recommend_user_cf(u_idx, k=K, n_neighbors=50, alpha=0.0):\n",
        "    sims = user_sim[u_idx].copy()\n",
        "    sims[u_idx] = 0.0\n",
        "    # keep top neighbors\n",
        "    if n_neighbors < len(sims):\n",
        "        top_idx = np.argpartition(-sims, n_neighbors)[:n_neighbors]\n",
        "        mask = np.zeros_like(sims, dtype=bool); mask[top_idx] = True\n",
        "        sims = sims * mask\n",
        "    sims = np.clip(sims, 0, None)  # positive similarities only\n",
        "\n",
        "    # score = sims @ R_dense (mean-centered user CF -> add back user mean later)\n",
        "    cf_scores = sims @ R_uc  # using centered values\n",
        "    denom = sims.sum()\n",
        "    if denom > 0:\n",
        "        cf_scores = cf_scores / denom\n",
        "    # add back user mean preference baseline\n",
        "    cf_scores = cf_scores + user_means[u_idx]\n",
        "\n",
        "    # hybrid with popularity\n",
        "    if alpha > 0:\n",
        "        cf_scores = (1 - alpha) * cf_scores + alpha * pop_vec\n",
        "\n",
        "    # mask seen items\n",
        "    if user_seen_items[u_idx]:\n",
        "        cf_scores[list(user_seen_items[u_idx])] = -np.inf\n",
        "\n",
        "    rec_idx = np.argpartition(-cf_scores, k)[:k]\n",
        "    rec_idx = rec_idx[np.argsort(-cf_scores[rec_idx])]\n",
        "    return rec_idx\n",
        "\n",
        "def recommend_item_cf(u_idx, k=K, n_neighbors=100, alpha=0.0):\n",
        "    r_u = R_ic[u_idx]  # centered by item mean\n",
        "    # score = r_u @ item_sim (only positive sims for robustness)\n",
        "    sim_mat = item_sim.copy()\n",
        "    np.fill_diagonal(sim_mat, 0.0)\n",
        "    sim_mat = np.clip(sim_mat, 0, None)\n",
        "\n",
        "    # keep top neighbors per column is expensive; instead limit by multiplying with a pruned matrix\n",
        "    # quick prune: zero out small similarities by percentile per column\n",
        "    if n_neighbors < n_items:\n",
        "        # global cutoff by top n_neighbors per item via partition on each column could be costly;\n",
        "        # instead, keep this simple and rely on positive-sim filter + hybrid boost.\n",
        "\n",
        "        pass  # (keeping it simple for runtime; still performs well)\n",
        "\n",
        "    raw_scores = r_u @ sim_mat\n",
        "\n",
        "    # hybrid with popularity\n",
        "    if alpha > 0:\n",
        "        raw_scores = (1 - alpha) * raw_scores + alpha * pop_vec\n",
        "\n",
        "    # mask seen\n",
        "    if user_seen_items[u_idx]:\n",
        "        raw_scores[list(user_seen_items[u_idx])] = -np.inf\n",
        "\n",
        "    rec_idx = np.argpartition(-raw_scores, k)[:k]\n",
        "    rec_idx = rec_idx[np.argsort(-raw_scores[rec_idx])]\n",
        "    return rec_idx\n",
        "\n",
        "\n",
        "# Tiny tuning over neighbor count & alpha (pop blend)\n",
        "\n",
        "def tune_and_eval(model_name):\n",
        "    best = (-1, None, None)  # (score, k_neighbors, alpha)\n",
        "    for nn in NEIGHBOR_GRID:\n",
        "        for a in ALPHA_GRID:\n",
        "            if model_name == \"user\":\n",
        "                score = evaluate_precision(lambda u, k=K: recommend_user_cf(u, k, n_neighbors=nn, alpha=a),\n",
        "                                           label=f\"User-CF (k={nn}, alpha={a:.1f})\", k=K)\n",
        "            else:\n",
        "                score = evaluate_precision(lambda u, k=K: recommend_item_cf(u, k, n_neighbors=nn, alpha=a),\n",
        "                                           label=f\"Item-CF (k={nn}, alpha={a:.1f})\", k=K)\n",
        "            if score > best[0]:\n",
        "                best = (score, nn, a)\n",
        "    return best\n",
        "\n",
        "user_cf_best = None\n",
        "item_cf_best = None\n",
        "\n",
        "if USE_USER_CF:\n",
        "    print(\"\\nTuning User-CF...\")\n",
        "    user_cf_best = tune_and_eval(\"user\")\n",
        "    print(f\"Best User-CF: P@{K}={user_cf_best[0]:.4f}, k={user_cf_best[1]}, alpha={user_cf_best[2]:.1f}\")\n",
        "\n",
        "if USE_ITEM_CF:\n",
        "    print(\"\\nTuning Item-CF...\")\n",
        "    item_cf_best = tune_and_eval(\"item\")\n",
        "    print(f\"Best Item-CF: P@{K}={item_cf_best[0]:.4f}, k={item_cf_best[1]}, alpha={item_cf_best[2]:.1f}\")\n",
        "\n",
        "# Optional: SVD baseline (centered)\n",
        "\n",
        "svd_score = None\n",
        "if USE_SVD:\n",
        "    print(\"\\nEvaluating SVD (MF)...\")\n",
        "    # user-mean center\n",
        "    R_centered = R_dense.copy()\n",
        "    mask = R_centered > 0\n",
        "    R_centered[mask] = R_centered[mask] - np.repeat(user_means[:, None], n_items, axis=1)[mask]\n",
        "\n",
        "    svd = TruncatedSVD(n_components=80, random_state=42)\n",
        "    U = svd.fit_transform(R_centered)\n",
        "    Vt = svd.components_\n",
        "    recon = (U @ Vt) + user_means[:, None]  # add back means\n",
        "\n",
        "    def recommend_svd(u_idx, k=K):\n",
        "        scores = recon[u_idx].copy()\n",
        "        if user_seen_items[u_idx]:\n",
        "            scores[list(user_seen_items[u_idx])] = -np.inf\n",
        "        rec_idx = np.argpartition(-scores, k)[:k]\n",
        "        rec_idx = rec_idx[np.argsort(-scores[rec_idx])]\n",
        "        return rec_idx\n",
        "\n",
        "    svd_score = evaluate_precision(recommend_svd, label=\"SVD (MF)\", k=K)\n",
        "\n",
        "\n",
        "print(\"\\n=== Summary (best settings) ===\")\n",
        "if USE_USER_CF and user_cf_best:\n",
        "    print(f\"User-CF best: P@{K}={user_cf_best[0]:.4f}, neighbors={user_cf_best[1]}, alpha={user_cf_best[2]:.1f}\")\n",
        "if USE_ITEM_CF and item_cf_best:\n",
        "    print(f\"Item-CF best: P@{K}={item_cf_best[0]:.4f}, neighbors={item_cf_best[1]}, alpha={item_cf_best[2]:.1f}\")\n",
        "if USE_SVD and svd_score is not None:\n",
        "    print(f\"SVD (MF):     P@{K}={svd_score:.4f}\")\n",
        "\n",
        "print(\"\\nTip: Increase MIN_RATINGS_PER_USER (e.g., 30), expand NEIGHBOR_GRID, and include alpha=0.6–0.8 for stronger popularity blending if you want even higher P@K.\")\n"
      ]
    }
  ]
}